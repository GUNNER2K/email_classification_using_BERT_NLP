# email_classification_using_BERT_NLP

<h3>How does BERT work:</h3>

  <p>BERT, which stands for Bidirectional Encoder Representations from Transformers, is based on Transformers, a deep learning model in which every output element is connected to every input element, and the weightings between them are dynamically calculated based upon their connection. </p>


  <p>The goal of any given NLP technique is to understand human language as it is spoken naturally. In BERT's case, this typically means predicting a word in a blank. To do this, models typically need to train using a large repository of specialized, labeled training data. This necessitates laborious manual data labeling by teams of linguists.

  BERT, however, was pre-trained using only an unlabeled, plain text corpus (namely the entirety of the English Wikipedia, and the Brown Corpus). It continues to learn unsupervised from the unlabeled text and improve even as its being used in practical applications (ie Google search). Its pre-training serves as a base layer of "knowledge" to build from. From there, BERT can adapt to the ever-growing body of searchable content and queries and be fine-tuned to a user's specifications. This process is known as transfer learning.</p>

<h3> the BERT chart </h3>

![image](https://user-images.githubusercontent.com/95174361/184196921-c93aeb8c-23a3-4115-b365-0fd11fc4bb40.png)

<h3>What is BERT used for? </h3>

BERT is currently being used at Google to optimize the interpretation of user search queries. BERT excels at several functions that make this possible, including:

Sequence-to-sequence based language generation tasks such as:
Question answering
Abstract summarization
Sentence prediction
Conversational response generation
Natural language understanding tasks such as:
Polysemy and Coreference (words that sound or look the same but have different meanings) resolution
Word sense disambiguation
Natural language inference
Sentiment classification
